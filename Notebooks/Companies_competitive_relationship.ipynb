{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fa87f8-1daf-4029-bef5-3b0a1f6fc3f5",
   "metadata": {},
   "source": [
    "# Companies competitive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f434ddc9-fb7f-4bea-9304-7a03f6963ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "sys.path.append(\"../\")\n",
    "from wallstreet_quant.edgar_extractor import fetch_10K_and_10Q_filings, extract_items_from_filing\n",
    "from wallstreet_quant.edgar_ai import competitors_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57761edb-2430-484c-81c8-c1b695cd71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = pd.read_csv(\"russel1000.csv\")\n",
    "symbols = symbols['Ticker'].to_list()\n",
    "filings = defaultdict(list)\n",
    "symbols = ['TSLA', 'NVDA', 'AAPL', 'AMZN', 'GOOGL', 'MSFT', 'META', 'NFLX', 'AMD', 'INTC']\n",
    "for s in symbols:\n",
    "    try:\n",
    "        filings[s] = fetch_10K_and_10Q_filings(s, \"2023-01-01\", \"2025-6-6\",form=[\"10-K\"])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edd2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find actual section for Item 10\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "items_needed = ['1', '7']  \n",
    "curr = extract_items_from_filing(filings['META'][0], items_needed)\n",
    "for i in range(10):\n",
    "    nvda_competitors = competitors_analysis(\"META\", curr['1'] + \"\\n\\n\" + curr['7'], model=\"o3-mini\")\n",
    "    test.append(list(nvda_competitors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d31f08",
   "metadata": {},
   "source": [
    "# Creating a set of unique company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6943d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800f47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyDeduper:\n",
    "    \"\"\"Cluster near‑duplicate company names.\n",
    "\n",
    "    Only one public method: :py:meth:`dedupe`.\n",
    "    No module‑scope helper functions are exposed, preventing namespace clashes\n",
    "    with other canonicalisers you may already have.\n",
    "    \"\"\"\n",
    "\n",
    "    # ── class‑level constants (private) ──────────────────────────\n",
    "    _SUFFIXES = {\n",
    "        # English\n",
    "        \"inc\", \"incorporated\", \"corp\", \"corporation\", \"co\", \"company\", \"companies\",\n",
    "        \"ltd\", \"limited\", \"plc\", \"llc\", \"llp\", \"lp\",\n",
    "        # EU\n",
    "        \"gmbh\", \"kg\", \"ag\", \"kgaa\", \"se\",\n",
    "        # Romance\n",
    "        \"sa\", \"sas\", \"sarl\", \"spa\", \"srl\", \"sl\",\n",
    "        # NL / Nordics\n",
    "        \"bv\", \"nv\", \"ab\", \"oy\",\n",
    "        # APAC\n",
    "        \"pte\", \"pty\", \"bhd\", \"sdn bhd\", \"kk\",\n",
    "        # misc\n",
    "        \"as\", \"doo\",\n",
    "    }\n",
    "    _SUFFIX_RE = re.compile(r\"\\b(?:{})(?:[\\s\\.]|$)\".format(\"|\".join(_SUFFIXES)), re.I)\n",
    "    _ACRONYM_RE = re.compile(r\"^[A-Z]{2,5}$\")\n",
    "    _TICKER_RE  = re.compile(r\"^[A-Z]{1,5}$\")\n",
    "\n",
    "    # ── ctor ────────────────────────────────────────────────────\n",
    "    def __init__(self,\n",
    "                 ticker_map: Dict[str, str],\n",
    "                 *,\n",
    "                 embed_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                 k_neighbors: int = 10,\n",
    "                 cosine_th: float = 0.90,\n",
    "                 embed_model: Optional[SentenceTransformer] = None) -> None:\n",
    "        self._ticker_map = ticker_map\n",
    "        self._k = k_neighbors\n",
    "        self._cos_th = cosine_th\n",
    "        self._embed_model = embed_model or SentenceTransformer(embed_model_name)\n",
    "\n",
    "    # ── public API ──────────────────────────────────────────────\n",
    "    def dedupe(self, raw_names: List[str]) -> Tuple[List[List[str]], Dict[str, int], List[str]]:\n",
    "        \"\"\"Return *(clusters, name▶cid map, representatives)*.\"\"\"\n",
    "        # 1. Expand tickers\n",
    "        step1 = [self._expand_ticker(tok) for tok in raw_names]\n",
    "        # 2. Expand acronyms\n",
    "        step2 = [self._expand_acronym(tok, step1) if self._is_acronym(tok) else tok for tok in step1]\n",
    "        # 3. Bucket by canonical key\n",
    "        buckets: defaultdict[str, List[str]] = defaultdict(list)\n",
    "        for nm in step2:\n",
    "            buckets[self._canonicalise(nm)].append(nm)\n",
    "        reps = [names[0] for names in buckets.values()]\n",
    "        emb = self._embed(reps)\n",
    "        G = self._build_graph(emb)\n",
    "        return self._to_clusters(G, reps, buckets)\n",
    "\n",
    "    # ── private helpers (all names prefixed) ────────────────────\n",
    "    @staticmethod\n",
    "    def _normalise_unicode(text: str) -> str:\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def _canonicalise(self, name: str) -> str:\n",
    "        s = self._normalise_unicode(name)\n",
    "        s = self._SUFFIX_RE.sub(\" \", s.lower())\n",
    "        s = re.sub(r\"[^\\w ]\", \" \", s)\n",
    "        return \" \".join(sorted(s.split()))\n",
    "\n",
    "    def _is_acronym(self, token: str) -> bool:\n",
    "        return bool(self._ACRONYM_RE.fullmatch(token))\n",
    "\n",
    "    def _expand_ticker(self, token: str) -> str:\n",
    "        return self._ticker_map[token] if self._TICKER_RE.fullmatch(token) and token in self._ticker_map else token\n",
    "\n",
    "    def _expand_acronym(self, acr: str, universe: List[str]) -> str:\n",
    "        target = acr.upper()\n",
    "        for cand in universe:\n",
    "            if ''.join(w[0] for w in cand.split()).upper() == target:\n",
    "                return cand\n",
    "        return acr\n",
    "\n",
    "    def _embed(self, strings: List[str]) -> np.ndarray:\n",
    "        X = self._embed_model.encode(strings, convert_to_numpy=True)\n",
    "        return X / np.linalg.norm(X, axis=1, keepdims=True).clip(min=1e-9)\n",
    "\n",
    "    def _build_graph(self, X: np.ndarray) -> nx.Graph:\n",
    "        dim = X.shape[1]\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(X)\n",
    "        D, I = index.search(X, self._k)\n",
    "        # Fix: Create empty graph and add nodes explicitly\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(len(X)))\n",
    "        for i, (scores, idxs) in enumerate(zip(D, I)):\n",
    "            for score, j in zip(scores, idxs):\n",
    "                if j > i and score >= self._cos_th:\n",
    "                    G.add_edge(i, j)\n",
    "        return G\n",
    "\n",
    "    def _to_clusters(self,\n",
    "                     G: nx.Graph,\n",
    "                     reps: List[str],\n",
    "                     buckets: Dict[str, List[str]]) -> Tuple[List[List[str]], Dict[str, int], List[str]]:\n",
    "        clusters: List[List[str]] = []\n",
    "        name2cid: Dict[str, int] = {}\n",
    "        representatives: List[str] = []\n",
    "        for cid, comp in enumerate(nx.connected_components(G)):\n",
    "            members: List[str] = []\n",
    "            for rep_idx in comp:\n",
    "                key = self._canonicalise(reps[rep_idx])\n",
    "                members.extend(buckets[key])\n",
    "            clusters.append(members)\n",
    "            representatives.append(reps[next(iter(comp))])\n",
    "            for m in members:\n",
    "                name2cid[m] = cid\n",
    "        return clusters, name2cid, representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5ad6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name → cid → rep\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MSFT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName → cid → rep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m raw:\n\u001b[0;32m---> 12\u001b[0m     cid \u001b[38;5;241m=\u001b[39m \u001b[43mname2cid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps[cid]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClusters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MSFT'"
     ]
    }
   ],
   "source": [
    "raw = [\n",
    "    \"MSFT\", \"Microsoft\", \"Microsoft Corporation\",\n",
    "    \"Advanced Micro Devices\", \"AMD\",\n",
    "    \"Alphabet Inc.\", \"GOOGL\", \"Google\",\n",
    "    \"Bayer AG\", \"Bayer\"\n",
    "]\n",
    "ticker_map = {\"MSFT\": \"Microsoft Corporation\", \"AMD\": \"Advanced Micro Devices\", \"GOOGL\": \"Alphabet Inc.\"}\n",
    "deduper = CompanyDeduper(ticker_map)\n",
    "clusters, name2cid, reps = deduper.dedupe(raw)\n",
    "print(\"Name → cid → rep\")\n",
    "for n in raw:\n",
    "    cid = name2cid[n]\n",
    "    print(f\"{n:25} → {cid} → {reps[cid]}\")\n",
    "print(\"\\nClusters:\")\n",
    "for i, g in enumerate(clusters):\n",
    "    print(i, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dedac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Microsoft Corporation': 0,\n",
       " 'Microsoft': 0,\n",
       " 'Advanced Micro Devices': 1,\n",
       " 'Alphabet Inc.': 2,\n",
       " 'Google': 3,\n",
       " 'Bayer AG': 4,\n",
       " 'Bayer': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2799c15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Microsoft Corporation', 'Microsoft', 'Microsoft Corporation'],\n",
       " ['Advanced Micro Devices', 'Advanced Micro Devices'],\n",
       " ['Alphabet Inc.', 'Alphabet Inc.'],\n",
       " ['Google'],\n",
       " ['Bayer AG', 'Bayer']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
